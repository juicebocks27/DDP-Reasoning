{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the candidate predictions and reference sentences\n",
    "predictions = [\"hello there general kenobi\", \"foo bar foobar\"]\n",
    "references = [[\"hello there general kenobi\", \"hello there !\"],[\"foo bar foobar\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c17557ebe174251affa7d7d2c25d09f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9de6b45fce29428fb3c10579ee440267",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3ceb95d472d4423ab44820f5dd12710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 1.0, 'precisions': [1.0, 1.0, 1.0, 1.0], 'brevity_penalty': 1.0, 'length_ratio': 1.1666666666666667, 'translation_length': 7, 'reference_length': 6}\n"
     ]
    }
   ],
   "source": [
    "# Load the BLEU evaluation metric\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "# Compute the BLEU score\n",
    "results = bleu.compute(predictions=predictions, references=references)\n",
    "\n",
    "# Print the results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 1.0, 'rouge2': 1.0, 'rougeL': 1.0, 'rougeLsum': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Load the ROUGE evaluation metric\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "# Compute the ROUGE score\n",
    "results = rouge.compute(predictions=predictions, references=references)\n",
    "\n",
    "# Print the results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the candidate predictions and reference sentences (more complex example)\n",
    "predictions = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"This is a slightly different sentence.\",\n",
    "    \"A very long sentence with many words to test ROUGE.\",\n",
    "    \"Short and sweet.\"\n",
    "]\n",
    "\n",
    "references = [\n",
    "    [\n",
    "        \"The quick brown fox jumped over the lazy dogs.\",  # Slightly different wording\n",
    "        \"A fox jumps over a dog.\"  # More concise version\n",
    "    ],\n",
    "    [\n",
    "        \"This is a different sentence.\",  # Similar but different\n",
    "        \"A different sentence this is.\"  # Rearranged words\n",
    "    ],\n",
    "    [\n",
    "        \"A long sentence with lots of words.\",  # Similar but shorter\n",
    "        \"Many words in this long sentence to test ROUGE.\"  # Word order change\n",
    "    ],\n",
    "    [\n",
    "        \"Short and sweet.\",  # Exact match\n",
    "        \"A short sentence.\"  # Similar meaning\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.4593018255754881, 'precisions': [0.90625, 0.7142857142857143, 0.4583333333333333, 0.15], 'brevity_penalty': 1.0, 'length_ratio': 1.28, 'translation_length': 32, 'reference_length': 25}\n"
     ]
    }
   ],
   "source": [
    "# Load the BLEU evaluation metric\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "# Compute the BLEU score\n",
    "results = bleu.compute(predictions=predictions, references=references)\n",
    "\n",
    "# Print the results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.8559276980329612, 'rouge2': 0.6905637254901961, 'rougeL': 0.8187759952465834, 'rougeLsum': 0.8187759952465834}\n"
     ]
    }
   ],
   "source": [
    "# Load the ROUGE evaluation metric\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "# Compute the ROUGE score\n",
    "results = rouge.compute(predictions=predictions, references=references)\n",
    "\n",
    "# Print the results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the candidate predictions and reference sentences (more complex example)\n",
    "predictions = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"This is a slightly different sentence.\",\n",
    "    \"A very long sentence with many words to test ROUGE.\",\n",
    "    \"Short and sweet.\"\n",
    "]\n",
    "\n",
    "references = [\n",
    "    [\n",
    "        \"The quick brown fox jumped over the lazy dogs.\",  # Slightly different wording\n",
    "        \"A fox jumps over a dog.\"  # More concise version\n",
    "    ],\n",
    "    [\n",
    "        \"This is a different sentence.\",  # Similar but different\n",
    "        \"A different sentence this is.\"  # Rearranged words\n",
    "    ],\n",
    "    [\n",
    "        \"A long sentence with lots of words.\",  # Similar but shorter\n",
    "        \"Many words in this long sentence to test ROUGE.\"  # Word order change\n",
    "    ],\n",
    "    [\n",
    "        \"Short and sweet.\",  # Exact match\n",
    "        \"A short sentence.\"  # Similar meaning\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"llama RAG with explanaton responses\": \n",
    "predictions = [\"Let's analyze the given information:\\n\\n1. Eight persons namely P, Q, R, S, T, U, V, and W are sitting around a circular table facing the centre of the table but not necessarily in the same order.\\n\\n2. P sits second to the right of U, who sits opposite to T.\\n3. Two persons sit between Q and T.\\n4. S sits third to the right of P.\\n5. V sits second to the left of W.\\n\\nWe need to find out how many persons sit between P and R when counted from the left of P.\\n\\nLet's start with the given information:\\n\\n1. U sits opposite to T, so they are at the ends (i.e., 12 o'clock and 6 o'clock positions).\\n2. P sits second to the right of U. Since there are only eight people and P is two positions away from U, we can conclude that P is at the 3 o'clock position.\\n3. Two persons sit between Q and T. Given the circular arrangement, this means Q and T cannot be adjacent.\\n\\nNow, let's analyze the statement \\\"S sits third to the right of P.\\\" If S were sitting three positions away from P in a clockwise direction, it would place S at the 9 o'clock position. However, considering that V sits second to the left of W (i.e., V is between U and T), we can conclude that S must be between V and W.\\n\\nConsidering all the given information, let's visualize the arrangement:\\n\\nFrom P's perspective (at the 3 o'clock position):\\n- Q would sit two positions away from P (at the 5 o'clock or 7 o'clock position).\\n- R would sit one position to the right of T (implying that there must be at least three people between Q and T, which we can confirm using statement #3).\\n- Given that S is third to the right of P, it must sit after W.\\n\\nSince two persons sit between Q and T, it implies that they are not directly adjacent. Based on this information, R cannot sit immediately next to Q, and there has to be at least one person sitting between them (Q and R). Thus, when counted from the left of P, there is one person between P and R.\\n\\nThe answer is: One\",]\n",
    "# \"correct_answer\": \n",
    "references = [\"\\\"Three\\\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.0, 'precisions': [0.004329004329004329, 0.0, 0.0, 0.0], 'brevity_penalty': 1.0, 'length_ratio': 154.0, 'translation_length': 462, 'reference_length': 3}\n"
     ]
    }
   ],
   "source": [
    "# Load the BLEU evaluation metric\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "# Compute the BLEU score\n",
    "results = bleu.compute(predictions=predictions, references=references)\n",
    "\n",
    "# Print the results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.0050632911392405056, 'rouge2': 0.0, 'rougeL': 0.0050632911392405056, 'rougeLsum': 0.0050632911392405056}\n"
     ]
    }
   ],
   "source": [
    "# Load the ROUGE evaluation metric\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "# Compute the ROUGE score\n",
    "results = rouge.compute(predictions=predictions, references=references)\n",
    "\n",
    "# Print the results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"llama RAG with explanaton responses\": \n",
    "predictions = [\"The answer is: One\",]\n",
    "# \"correct_answer\": \n",
    "references = [\"\\\"Three\\\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.0, 'precisions': [0.0, 0.0, 0.0, 0.0], 'brevity_penalty': 1.0, 'length_ratio': 1.6666666666666667, 'translation_length': 5, 'reference_length': 3}\n"
     ]
    }
   ],
   "source": [
    "# Load the BLEU evaluation metric\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "# Compute the BLEU score\n",
    "results = bleu.compute(predictions=predictions, references=references)\n",
    "\n",
    "# Print the results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# Load the ROUGE evaluation metric\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "# Compute the ROUGE score\n",
    "results = rouge.compute(predictions=predictions, references=references)\n",
    "\n",
    "# Print the results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"llama RAG with explanaton responses\": \n",
    "predictions = [\"The answer is: Three\",]\n",
    "# \"correct_answer\": \n",
    "references = [\"Three\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.0, 'precisions': [0.2, 0.0, 0.0, 0.0], 'brevity_penalty': 1.0, 'length_ratio': 5.0, 'translation_length': 5, 'reference_length': 1}\n"
     ]
    }
   ],
   "source": [
    "# Load the BLEU evaluation metric\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "# Compute the BLEU score\n",
    "results = bleu.compute(predictions=predictions, references=references)\n",
    "\n",
    "# Print the results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.4, 'rouge2': 0.0, 'rougeL': 0.4, 'rougeLsum': 0.4}\n"
     ]
    }
   ],
   "source": [
    "# Load the ROUGE evaluation metric\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "# Compute the ROUGE score\n",
    "results = rouge.compute(predictions=predictions, references=references)\n",
    "\n",
    "# Print the results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"llama RAG with explanaton responses\": \n",
    "predictions = [\"Three\",]\n",
    "# \"correct_answer\": \n",
    "references = [\"Three\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.0, 'precisions': [1.0, 0.0, 0.0, 0.0], 'brevity_penalty': 1.0, 'length_ratio': 1.0, 'translation_length': 1, 'reference_length': 1}\n"
     ]
    }
   ],
   "source": [
    "# Load the BLEU evaluation metric\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "# Compute the BLEU score\n",
    "results = bleu.compute(predictions=predictions, references=references)\n",
    "\n",
    "# Print the results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 1.0, 'rouge2': 0.0, 'rougeL': 1.0, 'rougeLsum': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Load the ROUGE evaluation metric\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "# Compute the ROUGE score\n",
    "results = rouge.compute(predictions=predictions, references=references)\n",
    "\n",
    "# Print the results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
